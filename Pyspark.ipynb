{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c019557d",
   "metadata": {},
   "source": [
    "## SparkSession ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "524de807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"D:/spark-3.0.3-bin-hadoop2.7/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9871b571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab7db945",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"pyspark\").master('local').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13bf3a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1f797f41a50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b9a9291",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark2 = SparkSession.newSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd3c6fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function pyspark.sql.session.SparkSession.newSession(self)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65ceed77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000001F797F41A50> <function SparkSession.newSession at 0x000001F797F19F30>\n"
     ]
    }
   ],
   "source": [
    "print(spark,spark2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55eecd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark3 = SparkSession.builder.getOrCreate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1620ebde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000001F797F41A50>\n",
      "<function SparkSession.newSession at 0x000001F797F19F30>\n",
      "<bound method SparkSession.Builder.getOrCreate of <pyspark.sql.session.SparkSession.Builder object at 0x000001F797C16830>>\n"
     ]
    }
   ],
   "source": [
    "print(spark)\n",
    "print(spark2)\n",
    "print(spark3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4880371",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(\"Scala\", 25000), (\"Spark\", 35000), (\"PHP\", 21000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "014118df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: bigint]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87353103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1='Scala', _2=25000), Row(_1='Spark', _2=35000), Row(_1='PHP', _2=21000)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8285e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|   _1|   _2|\n",
      "+-----+-----+\n",
      "|Scala|25000|\n",
      "|Spark|35000|\n",
      "|  PHP|21000|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b54bab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Database(name='default', description='default database', locationUri='file:/F:/Data%20science/python/Pyspark/spark-warehouse')]\n"
     ]
    }
   ],
   "source": [
    "print(spark.catalog.listDatabases())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e8cae98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1f797f41a50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.getActiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5783c6",
   "metadata": {},
   "source": [
    "### commonly use SparkSession methods\n",
    "\n",
    "###### version() – Returns the Spark version where your application is running, probably the Spark version your cluster is configured with.\n",
    "\n",
    "###### createDataFrame() – This creates a DataFrame from a collection and an RDD\n",
    "\n",
    "###### getActiveSession() – returns an active Spark session.\n",
    "\n",
    "###### read() – Returns an instance of DataFrameReader class, this is used to read records from csv, parquet, avro, and more file formats into DataFrame.\n",
    "\n",
    "###### readStream() – Returns an instance of DataStreamReader class, this is used to read streaming data. that can be used to read streaming data into DataFrame.\n",
    "\n",
    "###### sparkContext() – Returns a SparkContext.\n",
    " \n",
    "###### sql() – Returns a DataFrame after executing the SQL mentioned.\n",
    "\n",
    "###### sqlContext() – Returns SQLContext.\n",
    "\n",
    "###### stop() – Stop the current SparkContext.\n",
    "\n",
    "###### table() – Returns a DataFrame of a table or view.\n",
    "\n",
    "###### udf() – Creates a PySpark UDF to use it on DataFrame, Dataset, and SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818796e5",
   "metadata": {},
   "source": [
    "## sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2356dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.3'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5538440e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=pyspark>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eae1b8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5223b995",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize([('java',0),('python',1),('scala',2),('sql',3),('javascript',4),('go',5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d4017d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[13] at readRDDFromFile at PythonRDD.scala:262"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "242b8b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[13] at readRDDFromFile at PythonRDD.scala:262\n"
     ]
    }
   ],
   "source": [
    "print(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2031ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('java', 0),\n",
       " ('python', 1),\n",
       " ('scala', 2),\n",
       " ('sql', 3),\n",
       " ('javascript', 4),\n",
       " ('go', 5)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b514c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = rdd.toDF(['language','serial_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9e9b9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|  language|serial_number|\n",
      "+----------+-------------+\n",
      "|      java|            0|\n",
      "|    python|            1|\n",
      "|     scala|            2|\n",
      "|       sql|            3|\n",
      "|javascript|            4|\n",
      "|        go|            5|\n",
      "+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2a737db",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = spark.sparkContext.range(1,5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a03e79e8",
   "metadata": {},
   "source": [
    "rdd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47059828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[29] at RDD at PythonRDD.scala:53\n"
     ]
    }
   ],
   "source": [
    "print(rdd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b9b02cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8a6ee8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "# rdd  = spark.sparkContext.parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7e18bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c4dd4d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd1) == type(rdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6e2f2a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "11225499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local-1660039371610\n",
      "3.0.3\n",
      "http://host.docker.internal:4040\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.applicationId)\n",
    "print(spark.sparkContext.version)\n",
    "print(spark.sparkContext.uiWebUrl) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14454e71",
   "metadata": {},
   "source": [
    "### when ever we use parallelize method it creates rdd and when we create it with range method it creates pipelined rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd5c3cb",
   "metadata": {},
   "source": [
    "###  commonly used sparkContext methods\n",
    "\n",
    "#### accumulator(value[, accum_param]) \n",
    "#### broadcast(value) \n",
    "#### emptyRDD()\n",
    "#### getOrCreate() \n",
    "#### hadoopFile() \n",
    "#### newAPIHadoopFile() \n",
    "#### sequenceFile() \n",
    "#### setLogLevel() \n",
    "#### textFile()\n",
    "#### union() \n",
    "#### wholeTextFiles()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01176135",
   "metadata": {},
   "source": [
    "# RDD Concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9ffb5df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd4 = spark.sparkContext.textFile(\"C:/Users/W10/Desktop/salary.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a9d6d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e5e33c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.api.java.JavaPairRDD@70fdddef"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.wholeTextFiles(\"C:/Users/W10/Desktop/salary.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "532e12b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "emptRDD = spark.sparkContext.emptyRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45b766e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8901a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd5 = spark.sparkContext.textFile(\"C:/Users/W10/Desktop/information.txt\",5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c89e30ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd5.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f83ec7c",
   "metadata": {},
   "source": [
    "### sometimes we want to repartition the rdd, for that pyspark provides two methods\n",
    "### 1. repartition()\n",
    "### 2. coalesce()\n",
    "\n",
    "#### repartition shuffle data from all the nodes it is very expensive execution it is also called full shuffle.\n",
    "#### coalesce() shuffle data from minimum nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1d03522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd6 = rdd5.flatMap(lambda x: x.split(\" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7cf6ddec",
   "metadata": {},
   "outputs": [],
   "source": [
    "numrdd = spark.sparkContext.parallelize(range(20),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f9640eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numrdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2a2e7951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[45] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numrdd.map(lambda x: (x,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "117c889a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd7 = numrdd.map(lambda x: x+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6731bc9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd7.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c7a90c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd7.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fea4dcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd8 = spark.sparkContext.parallelize([('jack',1),('mack',2),('sack',5),('jack',6),('jack',7),('lonare',8),('dongre',9),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "786c7b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd9 = rdd8.reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "35acb47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jack', 14), ('mack', 2), ('sack', 5), ('lonare', 8), ('dongre', 9)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd9.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a1abb738",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd10 = rdd9.filter(lambda x: x[1]<5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d9994bf3",
   "metadata": {},
   "source": [
    "rdd10.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ee6b6e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mack', 2)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd10.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "63d15725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd10.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "378c74d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd6.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a116dacc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['collections',\n",
       " '',\n",
       " 'their',\n",
       " 'rolls',\n",
       " 'is',\n",
       " 'to',\n",
       " 'store',\n",
       " 'the',\n",
       " 'data',\n",
       " '',\n",
       " '',\n",
       " 'VARRAYS',\n",
       " '\\tarrays',\n",
       " '\\tpre-defined',\n",
       " 'size',\n",
       " '--',\n",
       " 'we',\n",
       " 'have',\n",
       " 'to',\n",
       " 'predefind',\n",
       " 'the',\n",
       " 'array',\n",
       " 'size',\n",
       " '\\tindex',\n",
       " 'starts',\n",
       " 'with',\n",
       " '1',\n",
       " '\\tcannot',\n",
       " 'delete',\n",
       " 'element',\n",
       " '',\n",
       " 'nested',\n",
       " 'table',\n",
       " '\\tList',\n",
       " '\\tvariable',\n",
       " 'size',\n",
       " '--',\n",
       " 'we',\n",
       " \"don't\",\n",
       " 'need',\n",
       " 'to',\n",
       " 'specify',\n",
       " 'the',\n",
       " 'size',\n",
       " '\\tindex',\n",
       " 'starts',\n",
       " 'with',\n",
       " '1',\n",
       " '',\n",
       " 'associative',\n",
       " 'arrays',\n",
       " '\\tindexing',\n",
       " 'can',\n",
       " 'be',\n",
       " 'done',\n",
       " 'with',\n",
       " 'string',\n",
       " '\\tmap',\n",
       " '(hashmap)']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd6.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366e1268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
